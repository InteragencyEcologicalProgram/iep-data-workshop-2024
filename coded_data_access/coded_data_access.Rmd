---
title: "Coded data access in R"
author: "Dave Bosworth<br>CA Department of Water Resources"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  html_document: 
    code_folding: show
    toc: true
    toc_float:
      collapsed: false
editor_options: 
  chunk_output_type: console
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = here::here("docs"),
      envir = globalenv()
    )
    })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load tidyverse}
library(tidyverse)
```


# Simple Methods

## `read_csv()` with URL

```{r read csv dayflow, message = FALSE}
library(readr)

# Import data from a URL using read_csv()
url_dayflow_2023 <- "https://data.cnra.ca.gov/dataset/06ee2016-b138-47d7-9e85-f46fae674536/resource/f7c1ba7f-bd64-4762-88e3-6db9b2501b38/download/dayflowcalculations2023.csv"
df_dayflow_2023 <- read_csv(url_dayflow_2023)
df_dayflow_2023
```

## `download.file()` then import

```{r read csv edi, warning = FALSE, message = FALSE}
# Import data from EDI to temporary directory
url_edi_emp_2022 <- "https://portal.edirepository.org/nis/dataviewer?packageid=edi.458.10&entityid=cf231071093ac2861893793517db26f3"
download.file(url_edi_emp_2022, file.path(tempdir(), "EMP_DWQ_1975_2022.csv"), mode = "wb")

# Then import data using read_csv()
df_emp_2022 <- read_csv(file.path(tempdir(), "EMP_DWQ_1975_2022.csv"))
df_emp_2022
```


# Dedicated R Packages

## `EDIutils`

Package documentation: <https://docs.ropensci.org/EDIutils/> <br>
Available on CRAN: <https://cloud.r-project.org/web/packages/EDIutils/index.html>

### List data package revisions

```{r ediutils package revisions}
library(EDIutils)
edi_scope <- "edi"
edi_emp_id <- 458

list_data_package_revisions(scope = edi_scope, identifier = edi_emp_id)

edi_emp_rev <- list_data_package_revisions(
  scope = edi_scope, 
  identifier = edi_emp_id, 
  filter = "newest"
)
edi_emp_pid <- paste(edi_scope, edi_emp_id, edi_emp_rev, sep = ".")
edi_emp_pid
```

### List data entities

```{r ediutils data entities}
df_edi_emp_ent <- read_data_entity_names(packageId = edi_emp_pid)
df_edi_emp_ent
```

### Import data entity

```{r ediutils import data, message = FALSE}
edi_emp_ent_id <- df_edi_emp_ent %>% 
  filter(entityName == "EMP_DWQ_1975_2022") %>% 
  pull(entityId)

raw_emp_2022_edi <- read_data_entity(packageId = edi_emp_pid, entityId = edi_emp_ent_id)

df_emp_2022_edi <- read_csv(raw_emp_2022_edi)
df_emp_2022_edi
```


## `dataRetrieval`

Package documentation: <https://doi-usgs.github.io/dataRetrieval/> <br>
Available on CRAN: <https://cloud.r-project.org/web/packages/dataRetrieval/index.html>

Sacramento River at Freeport CA (USGS station 11447650): <https://waterdata.usgs.gov/monitoring-location/11447650/>

### NWIS Web Services

#### Station Information

```{r usgs station info nwis}
library(dataRetrieval)

df_srf_sta_info <- whatNWISsites(sites = "11447650")
glimpse(df_srf_sta_info)
```

#### Data availability

```{r usgs data availability nwis}
df_srf_uv_data_avail <- whatNWISdata(siteNumber = "11447650", service = "uv")
glimpse(df_srf_uv_data_avail)
```

#### USGS parameter codes

```{r usgs param code info}
as_tibble(parameterCdFile)

srf_uv_parm_cd <- unique(df_srf_uv_data_avail$parm_cd)
srf_uv_parm_cd

df_srf_uv_parm_cd <- as_tibble(parameterCdFile) %>% 
  filter(parameter_cd %in% srf_uv_parm_cd) %>% 
  select(parameter_cd, parameter_nm, parameter_units)
df_srf_uv_parm_cd

as_tibble(df_srf_uv_data_avail) %>% 
  select(parm_cd, loc_web_ds, begin_date, end_date, count_nu) %>% 
  left_join(df_srf_uv_parm_cd, by = join_by(parm_cd == parameter_cd))
```

#### Continuous data - instantaneous

```{r usgs rtm instantaneous}
df_srf_spc_inst <- readNWISuv(
  siteNumbers = "11447650",
  parameterCd = "00095", 
  startDate = "2023-01-01", endDate = "2023-12-31", 
  tz = "America/Los_Angeles"
)
as_tibble(df_srf_spc_inst)
```

#### Continuous data - daily averages

```{r usgs rtm daily avg}
df_srf_tfq_davg <- readNWISdv(
  siteNumbers = "11447650",
  parameterCd = "72137", 
  startDate = "2023-01-01", endDate = "2023-12-31"
)
as_tibble(df_srf_tfq_davg)
```

### WQP Web Services

#### Station Information

```{r usgs station info wqp}
whatWQPsites(siteid = "USGS-11447650")
```

#### Data availability

```{r usgs data availability wqp}
df_srf_nutr_data_avail <- readWQPsummary(
  siteid = "USGS-11447650", 
  characteristicType = "Nutrient"
)
df_srf_nutr_data_avail

df_srf_nutr_data_avail %>% summarize(
  min_yr = min(YearSummarized),
  max_yr = max(YearSummarized),
  num_samp = sum(ResultCount),
  .by = CharacteristicName
)
```

#### Discrete WQ data

Import discrete nitrate data from the Water Quality Portal (WQP)

```{r usgs discrete wqp}
df_srf_nitrate <- readWQPqw(
  siteNumbers = "USGS-11447650",
  parameterCd = "Nitrate", 
  startDate = "2023-01-01", endDate = "2023-12-31", 
  tz = "America/Los_Angeles"
)
as_tibble(df_srf_nitrate) %>% 
  select(MonitoringLocationIdentifier, ActivityStartDateTime,
    CharacteristicName, USGSPCode, ResultMeasureValue)
```

### Other Information

More useful links for accessing USGS data through `dataRetrieval`:

* Parameter codes: <https://help.waterdata.usgs.gov/codes-and-parameters/parameters>
* Stat (Statistic) codes: <https://help.waterdata.usgs.gov/code/stat_cd_nm_query?stat_nm_cd=%25&fmt=html>

#### USGS County and State codes

```{r usgs county state codes}
as_tibble(countyCd)
as_tibble(stateCd)
```


## `cder`

Package documentation: <https://hydroecology.net/cder/index.html> <br>
Available on CRAN: <https://cran.r-project.org/web/packages/cder/index.html>

### Station Information

```{r load cder}
library(cder)
```

```{r cder station info, eval = FALSE}
cdec_meta(station = "FPT")
```

### Import data

```{r cder import data}
df_fpt_turb <- cdec_query(
  stations = "FPT", sensors = 221, durations = "E", 
  start.date = "2024-03-01", end.date = "2024-03-31"
)
df_fpt_turb
```


# IEP Integrated Datasets

## `deltafish`

Available on GitHub: <https://github.com/Delta-Stewardship-Council/deltafish> <br>
EDI data repository: <https://portal.edirepository.org/nis/mapbrowse?scope=edi&identifier=1075>

```{r deltafish}
# install.packages("devtools")
# devtools::install_github("Delta-Stewardship-Council/deltafish")
library(deltafish)
# Build the database - this takes a while, use update = TRUE to 
  # re-build cached database
create_fish_db()

# Open two data files
surv <- open_survey()
fish <- open_fish()

# Filter for sources and taxa of interest and join them together
surv_FMWT <- surv %>% filter(Source == "FMWT") %>% select(SampleID, Date)

fish_smelt <- fish %>% 
  filter(Taxa %in% c("Dorosoma petenense", "Morone saxatilis", "Spirinchus thaleichthys"))

df_fish <- left_join(surv_FMWT, fish_smelt)

# Collect the resulting data frame - collect executes
  # the SQL query and gives you a table
df_fish_c <- collect(df_fish)
df_fish_c
```


## `zooper`

Available on GitHub: <https://github.com/InteragencyEcologicalProgram/zooper> <br>
EDI data repository: <https://portal.edirepository.org/nis/mapbrowse?scope=edi&identifier=539>

```{r zooper}
# install.packages("devtools")
# devtools::install_github("InteragencyEcologicalProgram/zooper")
library(zooper)

df_zoop <- Zoopsynther(
  Data_type = "Community", Response = c("CPUE", "BPUE"),
  Sources = c("EMP", "FRP", "FMWT"), Size_class = "Meso",
  Date_range = c("1990-10-01", "2000-09-30")
)
df_zoop
```


## `discretewq`

Available on GitHub: <https://github.com/InteragencyEcologicalProgram/discretewq> <br>
EDI data repository: <https://portal.edirepository.org/nis/mapbrowse?scope=edi&identifier=731>

```{r discretewq}
# install.packages("devtools")
# devtools::install_github("InteragencyEcologicalProgram/discretewq")
library(discretewq)

df_dwq <- wq(
  Sources = c("EMP", "NCRO", "USGS_CAWSC", "USGS_SFBS"), 
  Start_year = 2020, End_year = 2022
)
df_dwq
```


## `deltamapr`

Available on GitHub: <https://github.com/InteragencyEcologicalProgram/deltamapr>

```{r deltamapr}
# install.packages("devtools")
# devtools::install_github("InteragencyEcologicalProgram/deltamapr")
library(deltamapr)
library(sf)
WW_Delta
```

```{r wwdelta map}
ggplot(WW_Delta) + geom_sf() + theme_bw()
```

The following is additional info that's useful but not included in tutorial.

# Web Scraping

Package documentation: <https://rvest.tidyverse.org/index.html> <br>
Available on CRAN: <https://cloud.r-project.org/web/packages/rvest/index.html>

```{r web scraping}
library(rvest)

url_pforb <- "https://invasions.si.edu/nemesis/species_summary/-218"
html_pforb <- read_html(url_pforb)
html_elements(html_pforb, "table")

df_pforb_traits <- html_elements(html_pforb, "table")[[3]] %>% html_table()
df_pforb_traits
```

Use the SelectorGadget tool to find CSS selectors on a webpage: <https://selectorgadget.com/>


# Extract data from .pdf

Package documentation: <https://docs.ropensci.org/pdftools/> <br>
Available on CRAN: <https://cloud.r-project.org/web/packages/pdftools/index.html>

```{r extract data from pdf}
library(pdftools)
# Download Jan 2024 Delta Outflow Computation report from the USBR website to
# the temporary R directory
url_usbr_dout <- "https://www.usbr.gov/mp/cvo/vungvari/dout0124.pdf"
download.file(url_usbr_dout, file.path(tempdir(), "Delta_Outflow_0124.pdf"), mode = "wb")
usbr_pdf_txt <- pdf_text(file.path(tempdir(), "Delta_Outflow_0124.pdf")) %>% read_lines()
usbr_pdf_txt

# Keep "rows" 12-42 in the data and convert to a matrix with 23 columns
usbr_pdf_mat <- usbr_pdf_txt[12:42] %>%
  str_squish() %>%
  str_split_fixed(pattern = " ", n = 23)
usbr_pdf_mat

# Keep the columns for Date, Export, and Outflow; rename them; convert to tibble
usbr_pdf_mat2 <- usbr_pdf_mat[,c(1, 16, 18)]
colnames(usbr_pdf_mat2) <- c("Date", "Export", "Outflow")
df_usbr_dout <- as_tibble(usbr_pdf_mat2)
df_usbr_dout
```


